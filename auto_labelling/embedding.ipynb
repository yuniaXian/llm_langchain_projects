{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", 0)\n",
    "\n",
    "# https://www.tizi365.com/topic/10092.html\n",
    "model_local_path ='/Users/jiayixian/projects/llm/llm/models/bge-large-en-v1.5'\n",
    "model = SentenceTransformer(model_local_path)\n",
    "label = \"\"\n",
    "data_path = f\"/Users/jiayixian/projects/llm/data/{label}.xlsx\" \n",
    "# sentences clustering\n",
    "df = pd.read_excel(data_path)\n",
    "lst_text = df['utterance'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "# https://www.tizi365.com/topic/10092.html\n",
    "model_local_path ='/Users/jiayixian/projects/llm/llm/models/bge-large-en-v1.5'\n",
    "#model = SentenceTransformer(model_local_path)\n",
    "data_path = \"\" \n",
    "# sentences clustering\n",
    "df = pd.read_excel(data_path)\n",
    "label_col = 'my_label'\n",
    "col = 'utterance'\n",
    "\n",
    "guides_lst = txt2lst(f\"/Users/jiayixian/projects/llm/llm/{label}.txt\")\n",
    "\n",
    "top_k_labels = df[label_col].value_counts().head(8).index.tolist() # Series\n",
    "print(top_k_labels)\n",
    "\n",
    "label_lst = top_k_labels\n",
    "# ['statement_balance', '.total_balance', 'balance_x', 'balance_issue_incorrect', \n",
    "# 'balance_issue_incorrect0', '.payment_balance_confirm']\n",
    "\n",
    "#mat_dict_combined, text_dict=get_txt_lst(model, label_lst, df)\n",
    "#label2st= get_label2st(label_lst, df, label_col='my_label', col='utterance')\n",
    "#mat_crx_dict = get_cross_matrix_dict(label_lst, label2st, mat_dict_combined['combined'])\n",
    "\n",
    "label_lst = []\n",
    "# label_lst = ['balance_issue_incorrect', 'balance_issue_incorrect0']\n",
    "samples = generate_label2samples(model, df, label_lst, topk=1, Is_combined=True, path=\"\", label_col='my_label', col='utterance') # text_dict, label2st, mat_dict_combined, mat_crx_dict, \n",
    "\n",
    "import json\n",
    "label_joined_name = \"_\".join(label_lst)\n",
    "\n",
    "path = \"/Users/jiayixian/projects/llm/llm/samples_dict.json\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    label2samples_dict=json.load(f)\n",
    "f.close()\n",
    "\n",
    "if label_joined_name not in label2samples_dict:\n",
    "    label2samples_dict[label_joined_name] = []\n",
    "label2samples_dict[label_joined_name].append(samples)\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    json.dump(label2samples_dict,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guides_lst = txt2lst(\"/Users/jiayixian/projects/llm/data/due_data.txt\")\n",
    "label_col = 'my_label'\n",
    "col = 'utterance'\n",
    "top_k_labels = df[label_col].value_counts().head(8).index.tolist() # Series\n",
    "print(top_k_labels)\n",
    "\n",
    "#txt_samples = [text_dict['.due_date'][idx] for idx in min_row]\n",
    "#m = get_similarity(guides_lst, txt_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "def txt2lst(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip().split('. ', 1)[1] if line.strip().split('. ', 1)[0].isdigit() else line.strip() for line in lines]\n",
    "        return lines\n",
    "\n",
    "def get_similarity(lst1, lst2):\n",
    "    embeddings_1 = model.encode(lst1, normalize_embeddings=True)\n",
    "    embeddings_2 = model.encode(lst2, normalize_embeddings=True)\n",
    "    similarity = embeddings_1 @ embeddings_2.T\n",
    "    return similarity\n",
    "\n",
    "def get_max_min_col_row(data,Is_same_label=False, Is_print=True):\n",
    "\n",
    "    N, M = data.shape[0], data.shape[1]\n",
    "    if Is_same_label:\n",
    "        data = data-np.eye(N,M)\n",
    "\n",
    "    aver_row = np.sum(data, axis=0)/data.shape[0]\n",
    "    aver_col = np.sum(data, axis=1)/data.shape[1]\n",
    "    aver = np.sum(data)/(data.shape[0]*data.shape[1])\n",
    "\n",
    "    \n",
    "    # Calculate max, min, and average for each row\n",
    "    max_per_row = np.max(data, axis=1)\n",
    "    min_per_row = np.min(data, axis=1)\n",
    "    avg_per_row = np.mean(data, axis=1)\n",
    "\n",
    "    # Calculate max, min, and average for each column\n",
    "    max_per_col = np.max(data, axis=0)\n",
    "    min_per_col = np.min(data, axis=0)\n",
    "    avg_per_col = np.mean(data, axis=0)\n",
    "\n",
    "    # Find the index of max and min for each row\n",
    "    max_idx_per_row = np.argmax(data, axis=1)\n",
    "    min_idx_per_row = np.argmin(data, axis=1)\n",
    "\n",
    "    # Find the index of max and min for each column\n",
    "    max_idx_per_col = np.argmax(data, axis=0)\n",
    "    min_idx_per_col = np.argmin(data, axis=0)\n",
    "\n",
    "    # Find the second largest value in each column\n",
    "    second_largest_in_columns = np.partition(data, -2, axis=0)[-2]\n",
    "\n",
    "    # Find the second largest value in each row\n",
    "    second_largest_in_rows = np.partition(data, -2, axis=1)[:, -2]\n",
    "    \n",
    "    if Is_same_label:\n",
    "        avg_per_col = np.sum(data, axis=0)/(N-1)\n",
    "        avg_per_row = np.sum(data, axis=1)/(M-1)\n",
    "        aver = np.sum(aver_row)/(N-1)\n",
    "        min_per_row = np.min(data + np.eye(N,M), axis=1)\n",
    "        min_per_col = np.min(data + np.eye(N,M), axis=0)\n",
    "        min_idx_per_row = np.argmin(data + np.eye(N,M), axis=1)\n",
    "        min_idx_per_col = np.argmin(data + np.eye(N,M), axis=0)\n",
    "        data = data+np.eye(N,M)\n",
    "    # Print the results\n",
    "    if Is_print:\n",
    "        print(f\"aver row: {aver_row}, aver col: {aver_col}, aver: {aver}\")\n",
    "        print(\"Max per row:\", max_per_row)\n",
    "        print(\"Min per row:\", min_per_row)\n",
    "        print(\"Average per row:\", avg_per_row)\n",
    "\n",
    "        print(\"Max per column:\", max_per_col)\n",
    "        print(\"Min per column:\", min_per_col)\n",
    "        print(\"Average per column:\", avg_per_col)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"Index of Max per row:\", max_idx_per_row)\n",
    "        print(\"Index of Min per row:\", min_idx_per_row)\n",
    "\n",
    "        print(\"Index of Max per column:\", max_idx_per_col)\n",
    "        print(\"Index of Min per column:\", min_idx_per_col)\n",
    "\n",
    "        print(\"Second largest value in each column:\", second_largest_in_columns)\n",
    "        print(\"Second largest value in each row:\", second_largest_in_rows)\n",
    "\n",
    "    d = {}\n",
    "    d['avg_r'] = avg_per_row\n",
    "    d['avg_c'] = avg_per_col\n",
    "    d['min_r'] = min_idx_per_row\n",
    "    d['min_c'] = min_idx_per_col\n",
    "    d['max_r'] = max_idx_per_row\n",
    "    d['max_c'] = max_idx_per_col\n",
    "    d['avg'] = aver\n",
    "    d['min'] = np.min(data)\n",
    "    d['max'] = np.max(data-np.eye(N,M)) if Is_same_label else np.max(data)\n",
    "    return d\n",
    "\n",
    "def get_dict_val2idx(A):\n",
    "    val2idx = defaultdict(list)\n",
    "    for idx, val in enumerate(A):\n",
    "        val2idx[int(val)].append(idx)\n",
    "    return val2idx\n",
    "\n",
    "def get_topk_within_idx(A, indices, topk):\n",
    "    selected_values = A[indices]\n",
    "\n",
    "    # Find the top 5 values and their corresponding indices\n",
    "    top_indices = selected_values.argsort()[-topk:][::-1]  # Indices of the top k values\n",
    "    top_values = selected_values[top_indices]\n",
    "\n",
    "    return top_values, [indices[i] for i in top_indices] # top_indices 是ndarray 不能直接 indices[top_indices]\n",
    "\n",
    "def get_topk_txt_within_idx(d_minmax, d_idx, topk=3, key='avg_r'):\n",
    "    A = d_minmax[key]\n",
    "    d_res = {}\n",
    "    for key, idxes in d_idx.items():\n",
    "        top_val, top_idxes = get_topk_within_idx(A, idxes, topk)\n",
    "        d_res[key] = top_idxes\n",
    "    return d_res\n",
    "\n",
    "def visual_res(d_res, text_dict, label1):\n",
    "    for key, idxes in d_res.items():\n",
    "        print(key, text_dict[label1][key])\n",
    "        print([[idx, text_dict[label1][idx]] for idx in idxes])\n",
    "\n",
    "def txt_collect(d_res, extra_list, d_txt4def, label):\n",
    "    idx_set = set(extra_list)\n",
    "    for outliner, txt_lst in d_res.items():\n",
    "        idx_set = idx_set | set(txt_lst)\n",
    "        idx_set.add(outliner)\n",
    "    d_txt4def[label] = sorted(list(idx_set))\n",
    "    return d_txt4def\n",
    "def get_txt_lst(label_lst, df, label_col='my_label', col='utterance'):\n",
    "    lst = []\n",
    "    text_dict = {}\n",
    "    for label in label_lst:\n",
    "        text_dict[label]=df[df[label_col]==label][col].tolist()\n",
    "        lst += df[df[label_col]==label][col].tolist()\n",
    "        print(label, len(text_dict[label]))\n",
    "    mat_dict = {}\n",
    "    mat_dict['combined'] = get_similarity(lst,lst)\n",
    "    \n",
    "    return mat_dict, text_dict\n",
    "\n",
    "def get_txt_lst_per_label(label_lst, df, top_k_labels, label_col='my_label', col='utterance'):\n",
    "    mat_dict = {}\n",
    "    for label in top_k_labels:\n",
    "        mat_dict[label] = get_similarity(df[df[label_col]==label][col].tolist(),df[df[label_col]==label][col].tolist())\n",
    "\n",
    "    text_dict = {}\n",
    "    for label in df['my_label'].unique():\n",
    "        text_dict[label]=df[df[label_col]==label][col].tolist()\n",
    "    \n",
    "    return mat_dict, text_dict\n",
    "\n",
    "\n",
    "def get_label2st(label_lst, df):\n",
    "    label2st = {}\n",
    "    s, t = 0, 0\n",
    "    for label in label_lst:\n",
    "        t = s+len(df[df[label_col]==label][col].tolist())\n",
    "        label2st[label]=[s, t]\n",
    "        print(f\"{label}: [{s}, {t}]\")\n",
    "        s = t\n",
    "    return label2st\n",
    "\n",
    "def get_label2matrix(matrix, label2st, label1, label2):\n",
    "    \"\"\"\n",
    "    matrinx: numpy ndarray of (N, N)\n",
    "    \"\"\"\n",
    "    st1, st2 = label2st[label1], label2st[label2]\n",
    "    return matrix[st1[0]:st1[1], st2[0]:st2[1]]\n",
    "\n",
    "def get_cross_matrix_dict(label_lst, label2st, matrix):\n",
    "    mat_dict = {}\n",
    "    for label1 in label_lst:\n",
    "        if label1 not in mat_dict:\n",
    "            mat_dict[label1] = {}\n",
    "            for label2 in label_lst:\n",
    "                mat_dict[label1][label2] = get_label2matrix(matrix, label2st, label1, label2)\n",
    "    return mat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#argsort(d_minmax['avg_r'])\n",
    "kth_smallest_index = np.argpartition(d_minmax['avg_r'], topk+3)[:topk+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39,  6, 37, 14])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kth_smallest_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lst = ['statement_balance', '.total_balance', 'balance_x', 'balance_issue_incorrect', 'balance_issue_incorrect0', '.payment_balance_confirm']\n",
    "mat_dict_combined, text_dict=get_txt_lst(label_lst, df)\n",
    "label2st= get_label2st(label_lst, df)\n",
    "mat_crx_dict = get_cross_matrix_dict(label_lst, label2st, mat_dict_combined['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label2samples(label_lst, topk=1, Is_combined=True, path=\"\"):\n",
    "    label1 = 'statement_balance'\n",
    "    label2 = '.total_balance' #'statement_balance'\n",
    "    aver_rel = {label:{} for label in label_lst}\n",
    "\n",
    "    if Is_combined:\n",
    "        pass\n",
    "    for l1 in label_lst:\n",
    "        for l2 in label_lst:\n",
    "            d_minmax = get_max_min_col_row(mat_crx_dict[l1][l2], Is_same_label=(l1==l2), Is_print=False)\n",
    "            aver_rel[l1][l2] = d_minmax\n",
    "            print(l1, l2, d_minmax['avg'])\n",
    "            #print(d_minmax['avg_r'])\n",
    "            #print(d_minmax['avg_c'])\n",
    "\n",
    "    #d_minmax = get_max_min_col_row(mat_crx_dict[label1][label2], Is_same_label=(label1==label2), Is_print=True)\n",
    "    # Get outliners\n",
    "    cnt = Counter(d_minmax['min_r'])\n",
    "    # Get value 2 idx dictionary\n",
    "    v2idx = get_dict_val2idx(A=d_minmax['min_r'])\n",
    "    # get the most centered ones with longest distance w.r.t outliners \n",
    "    # for each index in d_idx[val], get the aver_score, select the ones with higest aver_mean\n",
    "    d_res = get_topk_txt_within_idx(d_minmax, v2idx, topk, key='avg_r')\n",
    "    #print(d_minmax['min_r'])\n",
    "    visual_res(d_res, text_dict, label1)\n",
    "    sorted_outliners = [x[0] for x in sorted([(key, count) for key, count in cnt.items()], key=lambda x: -x[1])]\n",
    "    print(sorted_outliners)\n",
    "    d_txt4def = {}\n",
    "    kth_smallest_index = np.argpartition(d_minmax['avg_r'], topk+3)[:topk+3]\n",
    "    d_txt4def = txt_collect(d_res, kth_smallest_index, d_txt4def, label1)\n",
    "    samples = [text_dict[label1][idx] for idx in d_txt4def[label1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = 'statement_balance'\n",
    "label2 = '.total_balance' #'statement_balance'\n",
    "topk = 1\n",
    "aver_rel = {label:{} for label in label_lst}\n",
    "for l1 in label_lst:\n",
    "    for l2 in label_lst:\n",
    "        d_minmax = get_max_min_col_row(mat_crx_dict[l1][l2], Is_same_label=(l1==l2), Is_print=False)\n",
    "        aver_rel[l1][l2] = d_minmax\n",
    "        print(l1, l2, d_minmax['avg'])\n",
    "        #print(d_minmax['avg_r'])\n",
    "        #print(d_minmax['avg_c'])\n",
    "\n",
    "#d_minmax = get_max_min_col_row(mat_crx_dict[label1][label2], Is_same_label=(label1==label2), Is_print=True)\n",
    "# Get outliners\n",
    "cnt = Counter(d_minmax['min_r'])\n",
    "# Get value 2 idx dictionary\n",
    "v2idx = get_dict_val2idx(A=d_minmax['min_r'])\n",
    "# get the most centered ones with longest distance w.r.t outliners \n",
    "# for each index in d_idx[val], get the aver_score, select the ones with higest aver_mean\n",
    "d_res = get_topk_txt_within_idx(d_minmax, v2idx, topk, key='avg_r')\n",
    "#print(d_minmax['min_r'])\n",
    "visual_res(d_res, text_dict, label1)\n",
    "sorted_outliners = [x[0] for x in sorted([(key, count) for key, count in cnt.items()], key=lambda x: -x[1])]\n",
    "print(sorted_outliners)\n",
    "d_txt4def = {}\n",
    "kth_smallest_index = np.argpartition(d_minmax['avg_r'], topk+3)[:topk+3]\n",
    "d_txt4def = txt_collect(d_res, kth_smallest_index, d_txt4def, label1)\n",
    "samples = [text_dict[label1][idx] for idx in d_txt4def[label1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 101)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_dict_combined['combined'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statement_balance 17\n",
    ".total_balance 14\n",
    "balance_x 9\n",
    "balance_issue_incorrect 23\n",
    "balance_issue_incorrect0 24\n",
    ".payment_balance_confirm 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distant samples for extracting definitions\n",
    "\n",
    "label = '.due_date'\n",
    "label = 'combined'\n",
    "topk = 1\n",
    "d_minmax = get_max_min_col_row(mat_dict[label], Is_print=True)\n",
    "# Get outliners\n",
    "cnt = Counter(d_minmax['min_r'])\n",
    "# Get value 2 idx dictionary\n",
    "v2idx = get_dict_val2idx(A=d_minmax['min_r'])\n",
    "# get the most centered ones with longest distance w.r.t outliners \n",
    "# for each index in d_idx[val], get the aver_score, select the ones with higest aver_mean\n",
    "d_res = get_topk_txt_within_idx(d_minmax, v2idx, topk, key='avg_r')\n",
    "#print(d_minmax['min_r'])\n",
    "visual_res(d_res)\n",
    "sorted_outliners = [x[0] for x in sorted([(key, count) for key, count in cnt.items()], key=lambda x: -x[1])]\n",
    "print(sorted_outliners)\n",
    "d_txt4def = {}\n",
    "kth_smallest_index = np.argpartition(d_minmax['avg_r'], topk+3)[:topk+3]\n",
    "d_txt4def = txt_collect(d_res, kth_smallest_index, d_txt4def, label)\n",
    "samples = [text_dict[label][idx] for idx in d_txt4def[label]]\n",
    "samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max per row: [0.9999996  1.0000001  0.99999994 1.0000004  1.0000002  0.99999976\n",
      " 1.         1.0000007  0.99999976 1.0000001  1.0000004  1.0000004\n",
      " 1.0000002  1.0000004  0.9999997 ]\n",
      "Min per row: [0.6455203  0.4944928  0.53733385 0.64476454 0.4944928  0.5288431\n",
      " 0.6262151  0.5527522  0.60912544 0.5527522  0.668323   0.578886\n",
      " 0.5428655  0.62845504 0.5288431 ]\n",
      "Average per row: [0.7800027  0.6410429  0.6724819  0.7794098  0.6563986  0.724314\n",
      " 0.7242067  0.6689293  0.73428124 0.7136544  0.7904648  0.7355234\n",
      " 0.6711038  0.72936314 0.6485892 ]\n",
      "Max per column: [0.9999996  1.0000001  0.99999994 1.0000004  1.0000002  0.99999976\n",
      " 1.         1.0000007  0.99999976 1.0000001  1.0000004  1.0000004\n",
      " 1.0000002  1.0000004  0.9999997 ]\n",
      "Min per column: [0.6455203  0.4944928  0.53733385 0.64476454 0.4944928  0.5288431\n",
      " 0.6262151  0.5527522  0.60912544 0.5527522  0.668323   0.578886\n",
      " 0.5428655  0.62845504 0.5288431 ]\n",
      "Average per column: [0.7800027  0.6410429  0.6724819  0.7794098  0.6563986  0.724314\n",
      " 0.7242067  0.6689293  0.73428124 0.7136544  0.79046494 0.73552334\n",
      " 0.6711038  0.72936314 0.6485892 ]\n",
      "Index of Max per row: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "Index of Min per row: [14  4 14  4  1 14  2  9  7  7 12  1  1  9  5]\n",
      "Index of Max per column: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "Index of Min per column: [14  4 14  4  1 14  2  9  7  7 12  1  1  9  5]\n"
     ]
    }
   ],
   "source": [
    "m = get_similarity(samples, samples)\n",
    "d_samples = get_max_min_col_row(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nova-nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
